@article{Bouchard13,
  title = {Functional organization of human sensorimotor cortex for speech articulation},
  volume = {495},
  ISSN = {1476-4687},
  url = {http://dx.doi.org/10.1038/nature11911},
  DOI = {10.1038/nature11911},
  number = {7441},
  journal = {Nature},
  publisher = {Springer Science and Business Media LLC},
  author = {Bouchard,  Kristofer E. and Mesgarani,  Nima and Johnson,  Keith and Chang,  Edward F.},
  year = {2013},
  month = feb,
  pages = {327–332}
}

@article{Bouchard14,
  title = {Control of Spoken Vowel Acoustics and the Influence of Phonetic Context in Human Speech Sensorimotor Cortex},
  volume = {34},
  ISSN = {1529-2401},
  url = {http://dx.doi.org/10.1523/JNEUROSCI.1219-14.2014},
  DOI = {10.1523/jneurosci.1219-14.2014},
  number = {38},
  journal = {Journal of Neuroscience},
  publisher = {Society for Neuroscience},
  author = {Bouchard,  K. E. and Chang,  E. F.},
  year = {2014},
  month = sep,
  pages = {12662–12677}
}

@article{Carey17,
  title = {Functional and Quantitative MRI Mapping of Somatomotor Representations of Human Supralaryngeal Vocal Tract},
  ISSN = {1460-2199},
  url = {http://dx.doi.org/10.1093/cercor/bhw393},
  DOI = {10.1093/cercor/bhw393},
  journal = {Cerebral Cortex},
  publisher = {Oxford University Press (OUP)},
  author = {Carey,  Daniel and Krishnan,  Saloni and Callaghan,  Martina F. and Sereno,  Martin I. and Dick,  Frederic},
  year = {2017},
  month = jan 
}

@article{Chartier18,
title = {Encoding of Articulatory Kinematic Trajectories in Human Speech Sensorimotor Cortex},
journal = {Neuron},
volume = {98},
number = {5},
pages = {1042-1054.e4},
year = {2018},
issn = {0896-6273},
doi = {10.1016/j.neuron.2018.04.031},
url = {https://www.sciencedirect.com/science/article/pii/S0896627318303398},
author = {Josh Chartier and Gopala K. Anumanchipalli and Keith Johnson and Edward F. Chang},
}

@ARTICLE{Chen22,
  author={Chen, Sanyuan and Wang, Chengyi and Chen, Zhengyang and Wu, Yu and Liu, Shujie and Chen, Zhuo and Li, Jinyu and Kanda, Naoyuki and Yoshioka, Takuya and Xiao, Xiong and Wu, Jian and Zhou, Long and Ren, Shuo and Qian, Yanmin and Qian, Yao and Wu, Jian and Zeng, Michael and Yu, Xiangzhan and Wei, Furu},
  journal={IEEE Journal of Selected Topics in Signal Processing}, 
  title={WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing}, 
  year={2022},
  volume={16},
  number={6},
  pages={1505-1518},
  keywords={Predictive models;Self-supervised learning;Speech processing;Speech recognition;Convolution;Benchmark testing;Self-supervised learning;speech pre-training},
  doi={10.1109/JSTSP.2022.3188113}}

@article{Cheung16,
  title = {The auditory representation of speech sounds in human motor cortex},
  volume = {5},
  ISSN = {2050-084X},
  url = {http://dx.doi.org/10.7554/eLife.12577},
  DOI = {10.7554/elife.12577},
  journal = {eLife},
  publisher = {eLife Sciences Publications,  Ltd},
  author = {Cheung,  Connie and Hamilton,  Liberty S and Johnson,  Keith and Chang,  Edward F},
  year = {2016},
  month = mar 
}

@misc{Cho24,
      title={Coding Speech through Vocal Tract Kinematics}, 
      author={Cheol Jun Cho and Peter Wu and Tejas S. Prabhune and Dhruv Agarwal and Gopala K. Anumanchipalli},
      year={2024},
      eprint={2406.12998},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2406.12998}, 
}

@article{Conant18,
  title = {Human Sensorimotor Cortex Control of Directly Measured Vocal Tract Movements during Vowel Production},
  volume = {38},
  ISSN = {1529-2401},
  url = {http://dx.doi.org/10.1523/JNEUROSCI.2382-17.2018},
  DOI = {10.1523/jneurosci.2382-17.2018},
  number = {12},
  journal = {The Journal of Neuroscience},
  publisher = {Society for Neuroscience},
  author = {Conant,  David F. and Bouchard,  Kristofer E. and Leonard,  Matthew K. and Chang,  Edward F.},
  year = {2018},
  month = feb,
  pages = {2955–2966}
}


@article{Correia15,
  title = {Decoding Articulatory Features from fMRI Responses in Dorsal Speech Regions},
  volume = {35},
  ISSN = {1529-2401},
  url = {http://dx.doi.org/10.1523/JNEUROSCI.0977-15.2015},
  DOI = {10.1523/jneurosci.0977-15.2015},
  number = {45},
  journal = {The Journal of Neuroscience},
  publisher = {Society for Neuroscience},
  author = {Correia,  Joao M. and Jansma,  Bernadette M.B. and Bonte,  Milene},
  year = {2015},
  month = nov,
  pages = {15015–15025}}

@inproceedings{Eyben10,
author = {Eyben, Florian and W\"{o}llmer, Martin and Schuller, Bj\"{o}rn},
title = {Opensmile: the munich versatile and fast open-source audio feature extractor},
year = {2010},
isbn = {9781605589336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/1873951.1874246},
abstract = {We introduce the openSMILE feature extraction toolkit, which unites feature extraction algorithms from the speech processing and the Music Information Retrieval communities. Audio low-level descriptors such as CHROMA and CENS features, loudness, Mel-frequency cepstral coefficients, perceptual linear predictive cepstral coefficients, linear predictive coefficients, line spectral frequencies, fundamental frequency, and formant frequencies are supported. Delta regression and various statistical functionals can be applied to the low-level descriptors. openSMILE is implemented in C++ with no third-party dependencies for the core functionality. It is fast, runs on Unix and Windows platforms, and has a modular, component based architecture which makes extensions via plug-ins easy. It supports on-line incremental processing for all implemented features as well as off-line and batch processing. Numeric compatibility with future versions is ensured by means of unit tests. openSMILE can be downloaded from http://opensmile.sourceforge.net/.},
booktitle = {Proceedings of the 18th ACM International Conference on Multimedia},
pages = {1459–1462},
numpages = {4},
location = {Firenze, Italy},
series = {MM '10}
}

@article{Hamilton18,
author = {Hamilton, Liberty and Huth, Alexander},
year = {2018},
month = {07},
pages = {1-10},
title = {The revolution will not be controlled: natural stimuli in speech neuroscience},
volume = {35},
journal = {Language, Cognition and Neuroscience},
doi = {10.1080/23273798.2018.1499946}
}

@article{Hickok07,
author = {Hickok, Gregory and Poeppel, David},
year = {2007},
month = {06},
pages = {393-402},
title = {The Cortical Organization of Speech Processing},
volume = {8},
journal = {Nature reviews. Neuroscience},
doi = {10.1038/nrn2113}
}

@article{LeBel23,
  title = {A natural language fMRI dataset for voxelwise encoding models},
  volume = {10},
  ISSN = {2052-4463},
  url = {http://dx.doi.org/10.1038/s41597-023-02437-z},
  DOI = {10.1038/s41597-023-02437-z},
  number = {1},
  journal = {Scientific Data},
  publisher = {Springer Science and Business Media LLC},
  author = {LeBel,  Amanda and Wagner,  Lauren and Jain,  Shailee and Adhikari-Desai,  Aneesh and Gupta,  Bhavin and Morgenthal,  Allyson and Tang,  Jerry and Xu,  Lixiang and Huth,  Alexander G.},
  year = {2023},
  month = aug 
}

@Article{Li23,
author={Li, Yuanning
and Anumanchipalli, Gopala K.
and Mohamed, Abdelrahman
and Chen, Peili
and Carney, Laurel H.
and Lu, Junfeng
and Wu, Jinsong
and Chang, Edward F.},
title={Dissecting neural computations in the human auditory pathway using deep neural networks for speech},
journal={Nature Neuroscience},
year={2023},
month={Dec},
day={01},
volume={26},
number={12},
pages={2213-2225},
abstract={The human auditory system extracts rich linguistic abstractions from speech signals. Traditional approaches to understanding this complex process have used linear feature-encoding models, with limited success. Artificial neural networks excel in speech recognition tasks and offer promising computational models of speech processing. We used speech representations in state-of-the-art deep neural network (DNN) models to investigate neural coding from the auditory nerve to the speech cortex. Representations in hierarchical layers of the DNN correlated well with the neural activity throughout the ascending auditory system. Unsupervised speech models performed at least as well as other purely supervised or fine-tuned models. Deeper DNN layers were better correlated with the neural activity in the higher-order auditory cortex, with computations aligned with phonemic and syllabic structures in speech. Accordingly, DNN models trained on either English or Mandarin predicted cortical responses in native speakers of each language. These results reveal convergence between DNN model representations and the biological auditory pathway, offering new approaches for modeling neural coding in the auditory cortex.},
issn={1546-1726},
doi={10.1038/s41593-023-01468-4},
url={https://doi.org/10.1038/s41593-023-01468-4}
}

@article{Liberman85,
title = {The motor theory of speech perception revised},
journal = {Cognition},
volume = {21},
number = {1},
pages = {1-36},
year = {1985},
issn = {0010-0277},
doi = {10.1016/0010-0277(85)90021-6},
author = {Alvin M. Liberman and Ignatius G. Mattingly},
}

@article{Ramanarayanan13,
  title = {Spatio-temporal articulatory movement primitives during speech production: Extraction,  interpretation,  and validation},
  volume = {134},
  ISSN = {1520-8524},
  url = {http://dx.doi.org/10.1121/1.4812765},
  DOI = {10.1121/1.4812765},
  number = {2},
  journal = {The Journal of the Acoustical Society of America},
  publisher = {Acoustical Society of America (ASA)},
  author = {Ramanarayanan,  Vikram and Goldstein,  Louis and Narayanan,  Shrikanth S.},
  year = {2013},
  month = aug,
  pages = {1378–1394}
}

@InProceedings{Vaidya22,
  title = 	 {Self-Supervised Models of Audio Effectively Explain Human Cortical Responses to Speech},
  author =       {Vaidya, Aditya R and Jain, Shailee and Huth, Alexander},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {21927--21944},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  url = 	 {https://proceedings.mlr.press/v162/vaidya22a.html},
  abstract = 	 {Self-supervised language models are very effective at predicting high-level cortical responses during language comprehension. However, the best current models of lower-level auditory processing in the human brain rely on either hand-constructed acoustic filters or representations from supervised audio neural networks. In this work, we capitalize on the progress of self-supervised speech representation learning (SSL) to create new state-of-the-art models of the human auditory system. Compared against acoustic baselines, phonemic features, and supervised models, representations from the middle layers of self-supervised models (APC, wav2vec, wav2vec 2.0, and HuBERT) consistently yield the best prediction performance for fMRI recordings within the auditory cortex (AC). Brain areas involved in low-level auditory processing exhibit a preference for earlier SSL model layers, whereas higher-level semantic areas prefer later layers. We show that these trends are due to the models’ ability to encode information at multiple linguistic levels (acoustic, phonetic, and lexical) along their representation depth. Overall, these results show that self-supervised models effectively capture the hierarchy of information relevant to different stages of speech processing in human cortex.}
  doi = {10.48550/arXiv.2205.14252}
}

@misc{Vaidya22b,
author = {Vaidya, Aditya R},
title = {Speech encoding models: WavLM \& context},
howpublished = {Online},
month = {March},
year = {2022},
url = {https://github.com/HuthLab/lab-meeting-slides/blob/master/2022-Spring/20220309_wavlm-context.pdf}
}

@inproceedings{Wolf20,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Wolf, Thomas  and
      Debut, Lysandre  and
      Sanh, Victor  and
      Chaumond, Julien  and
      Delangue, Clement  and
      Moi, Anthony  and
      Cistac, Pierric  and
      Rault, Tim  and
      Louf, Remi  and
      Funtowicz, Morgan  and
      Davison, Joe  and
      Shleifer, Sam  and
      von Platen, Patrick  and
      Ma, Clara  and
      Jernite, Yacine  and
      Plu, Julien  and
      Xu, Canwen  and
      Le Scao, Teven  and
      Gugger, Sylvain  and
      Drame, Mariama  and
      Lhoest, Quentin  and
      Rush, Alexander",
    editor = "Liu, Qun  and
      Schlangen, David",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/2020.emnlp-demos.6",
    pages = "38--45",
}
